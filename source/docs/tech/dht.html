---
layout: page
navbar: Docs
no_header: true
footer: false
---

<h1>FogShare DHT</h1>

<p>In order to coordinate synchronizing data on multiple machines, FogSync uses
a distributed hash table based on Kademlia to store which machines store copies
of which synchronized shares.</p>

<h2>Node Address</h2>

<p>Each node has an address, which is its public IP address and UDP port. IPv4
addresses are stored in IPv6 format (::ffff:x.x.x.x), which means all DHT
addresses are 144 bits. Separate DHTs are run for IPv4 and IPv6, but they
should have the same data. (FIXME: How?)</p>

<h2>Node IDs</h2>

<p>Each node has a pseudorandom node ID that is generated using a Momentum-based 
proof of work function in order to make Sybil attacks more difficult. The ID is
a 256-bit value generated as follows:</p>

<p>Node IDs are good for an hour, and then must be regenerated. This means that
data must be re-inserted relitively frequently to stay fresh.</p>

<ol>
  <li>Let T = sha512(current 64-bit Unix time).</li>
  <li>Find A, B such that trunc(scrypt(T, A), M) == trunc(scrypt(T, B), M)</li>
  <li>Repeat until C = sha512(A + B + T) has at least N leading 0's.</li>
  <li>Your node ID is the last 256 bits of C.</li>
</ol>

<p>This generation function is tunable both by adjusting the parameters to
scrypt and by adjusting the truncation length M to force the use of a set
amount of memory, and adjusting the repeat factor N to make the whole process
take the right amount of computation.</p>

<p>Another node can check that you really jumped through this hoop comparitively
cheaply from your node address and your B and C. If you don't verify, everyone
else will ignore you.</p>

<h2>Data IDs</h2>

<p>Data stored in the table is identifed by its sha256 hash.</p>

<h2>Node Distance</h2>

<p>The distance between two Node IDs or a Node ID and a Data ID is defined by
the XOR of the two IDs interpreted as a 256 bit unsigned integer. Smaller is
closer.</p>

<h2>Routing Table</h2>

<p>Each node stores a routing table of known good nodes. This table is stored
as a sequence of buckets. Each bucket contains nodes from some part of the ID
space, identified by a bit prefix. When a bucket reaches it's maximum capacity
K, either it's full and no new nodes will be added or - if its bit prefix
matches that of the current node - it's split into two new buckets.</p>

<p>The whole table should be checked periodically to make sure the nodes
stay good.</p>

<h2>Good Nodes</h2>

<p>A node is good if it has responded to a query ever (it's not stuck behind a
bad firewall) and there has been any message from the node in the past 15
minutes (it's still up).</p>

<h2>Messages</h2>

<p>There are four basic messages in the protocol. Messages should fit in
a 512-byte UDP packet.</p>

<p>Each request has a Request ID, a randomly generated 128-bit value, that
identifies the conversation. This ID is retained in responses, including
an entire save/send/data/send/data sequence.</p>

<dl>
  <dt>ping</dt>
  <dd>
    <p> Request:  [ping, req-id, version-no]
    <br>Response: [pong, req-id, sender-Addr, responder-ID, responder-B, responder-C]</p>
    <p>A ping message checks if a node is still up. Requester should
       validate B and C. The inclusion of sender-Addr allows NATed nodes
       to determine their external address.</p>
  </dd>
  <dt>find</dt>
  <dd>
    <p> Request:    [find, req-id, data-ID]
    <br>Response A: [here, req-id, data-ID]
    <br>Response B: [look, req-id, [(node-ID, node-Addr]]</p>
    <p>Find requests should only be sent to validated hosts. If the node has
    the data, it should respond with it. Otherwise, it should respond with
    the six nearest peers.</p>
    <p>Find should be repeated until you either the "here" message or
    you perform K finds in a row without getting any closer peers.</p>
    <p>Once a requester gets a "here" message, they should make a "send"
    request.</p>
  </dd>
  <dt>save</dt>
  <dd>
    <p> Request:    [save, req-id, data-id, N]
    <br>Response A: [nope, req-id, "reason"]
    <br>Response B: [send, req-id, data-id, empty-bitmap]</p>
    <p>When storing data to the DHT, once the closest node has been found,
    a "save" request is sent. If the node agrees that it's the closest node
    to the data-id, it'll reply with a "send" message.</p>
  </dd>
  <dt>send</dt>
  <dd>
    <p> Request:    [send, req-id, data-id, bitmap]
    <br>Response A: [data, req-id, size, piece#, body] 
    <br>Response B: [nope, req-id, "reason"]</p>
    <p>Actual data transmission is requested by a "send" request. This
    command indicates which pieces of a data the recipient still needs
    in a bitfield. The recipient should send a data message for each
    unsent piece in the bitfield.</p>
    <p>The requester should resend the "send" request every 100ms or so
    with the latest bitfield until it gets all the pieces of data, at
    which point it should send a final "send" request with an empty
    bitfield to indicate that it has recieved the entire block of 
    data.</p>
  </dd>
</dl>

<h2>Normal Operation</h2>

<p>In normal operation, a client becomes a node by the following process:</p>

<ul>
  <li>Ping any node in the DHT, validate their ID.</li>
  <li>Send several random "find" requests to get an initial node population.</li>
  <li>Search for your node ID until you get K finds in a row without any closer
  peers.</li>
  <li>Validate each new node and insert into routing table as appropriate.</li>
</ul>

<p>Once you have a node population, you should perform the following update
cycle every 15 minutes.</p>

<ul>
  <li>Ping each node in your routing table that hasn't responded since the
  last cycle.</li>
  <li>Perform a "find" on each piece of data that you have stored and move
  it if there's a closer node.</li>
  <li>Perform a "find" on your ID to discover closer nodes.</li>
</ul>

<p>When you get a ping request from a new node, you should ping them back,
validate them, and add them to your routing table if they validate.</p>

<h2>Query-Only Operation</h2>

<p>Clients that are stuck behind a particularly obnoxious NAT or that are
operating on limited battery power can query the network without acting as a
storage node. To do this they perform a limited version of the normal
procedure:</p>

<ul>
  <li>Ping any node in the DHT, validate their ID.</li>
  <li>Send a "find" request to that node for your query.</li>
  <li>Repeat until you get the data.</li>
  <li>Store all nodes that respond for future queries.</li>
</ul>

<h2>What To Store</h2>

<p>Each user stores one record in the DHT, keyed by the SHA256 hash of their
PGP key fingerprint.</p>

<ul>
  <li>User PGP Key (Mine is 12k)</li>
  <li>Public Hosts (~ 20 bytes each)</li>
  <li>Encrypted Private Hosts (~ 20 bytes each)</li>
  <li>PGP Signature (~ 4k)</li>
</ul>

<p>This is sufficient information for new peers to bootstrap *if* they have
the user's PGP keypair.</p>

<p>A normal record should be about 18k, which means it'll take about 40 UDP
packets to transmit.</p>

<p>If each item is replicated 16 times and each user has one DHT node
up constantly, then the expected data storage requirement for a node is 
288k.</p>

<p>The maximum record size of 64k leaves space for about 2k hosts per user
which should be fine.</p>

<h2>References</h2>

<p>The BitTorrent DHT spec was especially useful in figuring out how a simple 
Kademila-ish DHT wants to work.</p>

<ul>
  <li><a href="http://www.bittorrent.org/beps/bep_0005.html">BitTorrent DHT</a></li>
  <li><a href="refs/maymounkov-kademlia-lncs.pdf">The Kademlia DHT</a></li>
  <li><a href="refs/MomentumProofOfWork.pdf">Momentum Proof Of Work Functions</a></li>
</ul>
